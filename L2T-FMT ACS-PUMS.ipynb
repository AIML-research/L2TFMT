{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3,0\" #list the gpu cores in the order you want to use e.g \"0,1,2,3\"\n",
    "cpu = torch.device('cpu')#\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dv=torch.device(\"cuda\")\n",
    "\n",
    "import os\n",
    "import torch.tensor as tensor\n",
    "os.chdir(\"/yourpath/ACS_PUMS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Train and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folktables\n",
    "from folktables import ACSDataSource\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data( download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\n",
    "        'AGEP',\n",
    "        'SCHL',\n",
    "        'MAR',\n",
    "        #'RELSHIPP',#'RELP',\n",
    "        'DIS',\n",
    "        'ESP',\n",
    "        'CIT',\n",
    "        'MIG',\n",
    "        'MIL',\n",
    "        'ANC',\n",
    "        'NATIVITY',\n",
    "        'DEAR',\n",
    "        'DEYE',\n",
    "        'DREM',\n",
    "        'SEX',\n",
    "        'RAC1P',\n",
    "        'PUMA',\n",
    "        'ST',\n",
    "        'OCCP',\n",
    "        'JWTR',#use 'JWTRNS' for testing (2019) data for training (2018) data the feature is 'JWTR',#\n",
    "        'POWPUMA',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employment = folktables.BasicProblem(\n",
    "     features=features,\n",
    "    target='ESR',\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='SEX',\n",
    "    preprocess=folktables.acs.adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Income = folktables.BasicProblem(\n",
    "     features=features,\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 50000,\n",
    "    group='SEX',\n",
    "    preprocess=folktables.acs.adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HealthInsurance = folktables.BasicProblem(\n",
    "     features=features,\n",
    "    target='HINS2',\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='SEX',\n",
    "    preprocess=folktables.acs.adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TravelTime = folktables.BasicProblem(\n",
    "     features=features,\n",
    "    target=\"JWMNP\",\n",
    "    target_transform=lambda x: x > 20,\n",
    "    group='SEX',\n",
    "    preprocess=folktables.acs.adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IncomePovertyRatio = folktables.BasicProblem(\n",
    "    features=features,\n",
    "    target='POVPIP',\n",
    "    target_transform=lambda x: x < 250,\n",
    "    group='SEX',\n",
    "    preprocess=folktables.acs.adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l1, g = Employment.df_to_numpy(acs_data)\n",
    "f, l2, g = Income.df_to_numpy(acs_data)\n",
    "\n",
    "f, l3, g = HealthInsurance.df_to_numpy(acs_data)\n",
    "f, l4, g = TravelTime.df_to_numpy(acs_data)\n",
    "f, l5, g = IncomePovertyRatio.df_to_numpy(acs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([[0 if v==False else 1 for v in l1],[0 if v==False else 1 for v in l2],[0 if v==False else 1 for v in l3],\\\n",
    "           [0 if v==False else 1 for v in l4],[0 if v==False else 1 for v in l5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=np.arange(len(f))\n",
    "X_train, X_val,in_tr,in_val  = train_test_split(f,ids, test_size=0.3,random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_v=[y[i][in_tr] for i in range(len(y))],[y[i][in_val] for i in range(len(y))]\n",
    "g_train=g[in_tr]\n",
    "g_val=g[in_val]\n",
    "N_tasks=len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[torch.tensor(y_train[i]) for i in range(N_tasks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build L2T-FMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL(nn.Module):\n",
    "\n",
    "    def __init__(self,d_in=50,tasks=2):\n",
    "        super(MTL, self).__init__()\n",
    "        self.tasks=tasks\n",
    "        self.fc1 = nn.Linear(d_in, 1024)  \n",
    "        self.bn1= nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fc4 = nn.Linear(1024,1024)\n",
    "        self.tasks_out=nn.ModuleDict({str(t):nn.Linear(128,2) for t in range(self.tasks)})\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc4(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))        \n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        t=[self.tasks_out[str(i)](x) for i in range(self.tasks)]\n",
    "        \n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_loss(output,target,x_control):\n",
    "    prot_att=x_control\n",
    "    index_prot=torch.squeeze(torch.nonzero(prot_att[:] != 1.))\n",
    "    target_prot=torch.index_select(target, 0, index=index_prot)\n",
    "    index_prot_pos=torch.squeeze(torch.nonzero(target_prot[:] == 1. ))\n",
    "    index_prot_neg=torch.squeeze(torch.nonzero(target_prot[:] == 0. ))\n",
    "\n",
    "    index_non_prot=torch.squeeze(torch.nonzero(prot_att[:] == 1.))\n",
    "    target_non_prot=torch.index_select(target, 0, index=index_non_prot)\n",
    "    index_non_prot_pos=torch.squeeze(torch.nonzero(target_non_prot[:] == 1. ))\n",
    "    index_non_prot_neg=torch.squeeze(torch.nonzero(target_non_prot[:] == 0. ))\n",
    "\n",
    "    l_prot_pos=F.cross_entropy(torch.index_select(output, 0, index=index_prot_pos),torch.index_select(target, 0, index=index_prot_pos))    \n",
    "    l_non_prot_pos=F.cross_entropy(torch.index_select(output, 0, index=index_non_prot_pos),torch.index_select(target, 0, index=index_non_prot_pos))    \n",
    "    l_non_prot_neg=F.cross_entropy(torch.index_select(output, 0, index=index_non_prot_neg),torch.index_select(target, 0, index=index_non_prot_neg))\n",
    "    l_prot_neg=F.cross_entropy(torch.index_select(output, 0, index=index_prot_neg),torch.index_select(target, 0, index=index_prot_neg))    \n",
    "\n",
    "    for l in [l_prot_pos,l_non_prot_pos,l_prot_neg,l_non_prot_neg]:\n",
    "        if torch.isinf(l)==True:\n",
    "            l=torch.zeros_like(l,requires_grad=True)\n",
    "    dl_pos=torch.max(l_prot_pos,l_non_prot_pos)\n",
    "    dl_neg=torch.max(l_prot_neg,l_non_prot_neg)\n",
    "    L=dl_pos+dl_neg\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "acc = torchmetrics.Accuracy()\n",
    "def DM_rate(output,target,x_control):\n",
    "    prot_att=x_control\n",
    "    index_prot=torch.squeeze(torch.nonzero(prot_att[:] != 1.))\n",
    "    target_prot=torch.index_select(target, 0, index=index_prot)\n",
    "    index_prot_pos=torch.squeeze(torch.nonzero(target_prot[:] == 1. ))\n",
    "    index_prot_neg=torch.squeeze(torch.nonzero(target_prot[:] == 0. ))\n",
    "\n",
    "    index_non_prot=torch.squeeze(torch.nonzero(prot_att[:] == 1.))\n",
    "    target_non_prot=torch.index_select(target, 0, index=index_non_prot)\n",
    "    index_non_prot_pos=torch.squeeze(torch.nonzero(target_non_prot[:] == 1. ))\n",
    "    index_non_prot_neg=torch.squeeze(torch.nonzero(target_non_prot[:] == 0. ))\n",
    "\n",
    "    if index_prot_pos.shape==torch.Size([]) or index_prot_pos.shape==torch.Size([0])\\\n",
    "        or index_non_prot_pos.shape==torch.Size([]) or index_non_prot_pos.shape==torch.Size([0]):\n",
    "            l_prot_pos=torch.tensor(0.0001)\n",
    "            l_non_prot_pos=torch.tensor(0.0001)\n",
    "    else:        \n",
    "            l_prot_pos=acc(torch.index_select(output, 0, index=index_prot_pos),torch.index_select(target, 0, index=index_prot_pos))    \n",
    "            l_non_prot_pos=acc(torch.index_select(output, 0, index=index_non_prot_pos),torch.index_select(target, 0, index=index_non_prot_pos))    \n",
    "    \n",
    "    if index_prot_neg.shape==torch.Size([]) or index_prot_neg.shape==torch.Size([0])\\\n",
    "        or index_non_prot_neg.shape==torch.Size([]) or index_non_prot_neg.shape==torch.Size([0]):\n",
    "            l_prot_neg=torch.tensor(0.0001)\n",
    "            l_non_prot_neg=torch.tensor(0.0001)\n",
    "    else:        \n",
    "            l_prot_neg=acc(torch.index_select(output, 0, index=index_prot_neg),torch.index_select(target, 0, index=index_prot_neg))    \n",
    "            l_non_prot_neg=acc(torch.index_select(output, 0, index=index_non_prot_neg),torch.index_select(target, 0, index=index_non_prot_neg))  \n",
    "            \n",
    "    dl_pos=torch.abs(l_prot_pos-l_non_prot_pos)\n",
    "    dl_neg=torch.abs(l_prot_neg-l_non_prot_neg)\n",
    "    DM=dl_pos+dl_neg\n",
    "    \n",
    "    return DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, fc1_dims, fc2_dims,fc3_dims=256,\n",
    "                 n_actions=2,n_tasks=2):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.tasks=n_tasks\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.bn1= nn.LayerNorm(fc1_dims)\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.bn2= nn.LayerNorm(fc2_dims)\n",
    "        self.fc3_dims = fc3_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.fc3_dims)\n",
    "        self.fc_out=nn.ModuleDict({str(t):nn.Linear(self.fc3_dims, self.n_actions) for t in range(self.tasks)})\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        Q_sa=[self.fc_out[str(i)](x) for i in range(self.tasks)]\n",
    "\n",
    "        return Q_sa#Q1_sa,Q2_sa#,Q3_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gamma=0.9\n",
    "omega_T=torch.tensor([1/N_tasks for t in range(N_tasks)]).to(dv)\n",
    "init_loss_T=[None for t in range(N_tasks)]\n",
    "criteria_t=nn.MSELoss()\n",
    "def Teacher(state,state_a,state_b,R_a,R_b,DqN,opti_T):\n",
    "    grads,G_n,loss_ratio=[],[],[]\n",
    "    global init_loss_T\n",
    "    global omega_T\n",
    "    opti_T.zero_grad()\n",
    "    for t in range(N_tasks):        \n",
    "        Q_S=DqN(state[t])[t]\n",
    "        target=[]\n",
    "        with torch.no_grad():\n",
    "            Q_sa=R_a[t]+gamma*torch.max(DqN(state_a[t])[t],1)[0]\n",
    "            Q_sb=R_b[t]+gamma*torch.max(DqN(state_b[t])[t],1)[0]\n",
    "        target=torch.tensor([[Q_sa[i],Q_sb[i]] for i in range(len(Q_sa))]).to(dv)\n",
    "        teach_loss=criteria_t(Q_S,target)\n",
    "        teach_loss.backward()\n",
    "        if init_loss_T[t]== None:\n",
    "            init_loss_T[t]=teach_loss.item()\n",
    "        loss_ratio.append(teach_loss.item()/init_loss_T[t])\n",
    "        grads_sh={}\n",
    "        for n,p in DqN.named_parameters():\n",
    "            if p.data.shape[0]!=2 and p.grad!=None:\n",
    "                grads_sh[n] = p.grad \n",
    "                \n",
    "        grads.append(grads_sh)\n",
    "        G_n.append(torch.linalg.norm(torch.stack([torch.linalg.norm(grads_sh[g]) for g in grads_sh])))\n",
    "    G_n = torch.stack(G_n)\n",
    "    E_t = sum(loss_ratio)/len(loss_ratio)\n",
    "    r_t=[loss/E_t for loss in loss_ratio] #relative inverse trainin g rate of teacher\n",
    "    omega_T,DqN = Update_model(DqN,grads,omega_T,G_n,r_t,opti_T)\n",
    "    torch.save(omega_T,'/yourpath/ACS-PUMS/omega_T.pt') # \n",
    "                   \n",
    "    return DqN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "def Update_model(model,grads_sh,omega,G_n,r_t,opti):\n",
    "    loss_gn=[(G_n[t]-torch.mean(G_n)*r_t[t]) for t in range(len(G_n))]\n",
    "    for i in range(len(G_n)):\n",
    "        d_l=0\n",
    "        if loss_gn[i]>0:\n",
    "            d_l+=(len(G_n)-1)/len(G_n)*G_n[i]\n",
    "        elif loss_gn[i]<0:\n",
    "            d_l-=(len(G_n)-1)/len(G_n)*G_n[i]\n",
    "        for j in range(len(G_n)):\n",
    "            if j!=i:\n",
    "                if loss_gn[j]>0:\n",
    "                    d_l-=(G_n[i]/len(G_n))\n",
    "                elif loss_gn[j]<0:\n",
    "                    d_l+=(G_n[i]/len(G_n))\n",
    "        \n",
    "        omega[i]-=lr*d_l\n",
    "        omega[i]=1\n",
    "    for t in range(len(G_n)):\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.data.shape[0]!=2 and p.grad!=None:\n",
    "                if t==0:\n",
    "                    p.grad=omega[t]*grads_sh[t][n]\n",
    "                else:\n",
    "                    p.grad+=omega[t]*grads_sh[t][n]\n",
    "                    \n",
    "    opti.step() \n",
    "    for i in range(len(omega)):\n",
    "        if omega[i]<0:\n",
    "            omega[i]=-omega[i]\n",
    "    return omega,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=MTL(model_pre.to(dv),tasks=N_tasks)\n",
    "net=nn.DataParallel(net)\n",
    "net.to(dv)\n",
    "opti_S=optim.AdamW(params=net.parameters())\n",
    "\n",
    "flat_w=torch.flatten(net.module.tasks_out['0'].weight)\n",
    "dqn=DeepQNetwork(len(flat_w),512,512,n_actions=2,n_tasks=N_tasks).to(dv)\n",
    "opti_T = optim.AdamW(dqn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Student(stud,path,action,X,y,t=0,xc=None):\n",
    "    stud.load_state_dict(torch.load(path))\n",
    "    opti = optim.Adam(stud.parameters())\n",
    "    opti.zero_grad()\n",
    "    out=stud(X)[t]\n",
    "    if xc!=None:\n",
    "        loss=action(out,y,xc)        \n",
    "    else:\n",
    "        loss=action(out,y)\n",
    "    #print(loss)\n",
    "    loss.backward()\n",
    "    opti.step()\n",
    "    return stud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "omega_S=torch.tensor([1/N_tasks for t in range(N_tasks)]).to(dv)\n",
    "init_loss_S=[None for t in range(N_tasks)]\n",
    "m_acc,flag=0.0,1\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "best_S=[[0,0] for t in range(N_tasks)]\n",
    "optim_path,optim_path_disc,optim_clas=[],[],[]\n",
    "path='/yourpath/ACS-PUMS/Model_l2tfmt.pt' #path to save your model\n",
    "path_t='/yourpath/ACS-PUMS/dqn_n.pt' ##path to save your teacher model\n",
    "torch.save(net.state_dict(),path)\n",
    "action_choices=[]\n",
    "stud=nn.DataParallel(MTL(model_pre,tasks=N_tasks)).to(dv)\n",
    "dt=[dataloader1,dataloader2,dataloader3,dataloader4]\n",
    "yt=[y_tr1,y_tr2,y_tr3,y_tr4]\n",
    "gt=[g_tr1,g_tr2,g_tr3,g_tr4]\n",
    "for epoch in range(20):  # loop over each NN multiple times\n",
    "    loss_pointer=[]\n",
    "    i,batch=0,8192\n",
    "    j=0\n",
    "    state_net,state_a,state_f=[[] for t in range(N_tasks)],[[] for t in range(N_tasks)],[[] for t in range(N_tasks)]\n",
    "    R_A,R_F=[[] for t in range(N_tasks)],[[] for t in range(N_tasks)]\n",
    "    while(i<len(X_train)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        if (i+batch)<len(X_train):\n",
    "            inputs, in_t = torch.tensor(X_train[i:i+batch]),in_tr[i:i+batch]\n",
    "            labels=[y_train[t][i:i+batch] for t in range(N_tasks)]#,y3_train[i:i+batch]]\n",
    "            #if epoch<pretrn:\n",
    "            xc=xg[i:i+batch]\n",
    "            i=i+batch \n",
    "        else:\n",
    "            inputs,in_t = torch.tensor(X_train[i:]),in_tr[i:]\n",
    "            labels=[y_train[t][i:] for t in range(N_tasks)]#,y2_train[i:]]#,y3_train[i:]]\n",
    "            #if epoch<pretrn:\n",
    "            xc=xg[i:]\n",
    "            i=len(X_train)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        net.load_state_dict(torch.load(path))\n",
    "        net.to(dv)\n",
    "        fg=[0 for i in range(N_tasks)]\n",
    "        with torch.no_grad():\n",
    "            for t in range(N_tasks):\n",
    "                flat=torch.flatten(net.module.tasks_out[str(t)].weight)\n",
    "                fg[t]=torch.argmax(dqn(flat)[t])\n",
    "                       \n",
    "        opti_S.zero_grad()\n",
    "        outputs = net(inputs.to(dv).float())\n",
    "        lp,grads,G_n,loss_ratio=[0 for t in range(N_tasks)],[],[],[]\n",
    "        for t in range(N_tasks):\n",
    "            \n",
    "            #MTL starts\n",
    "            loss_a=criteria(outputs[t], labels[t].to(dv))\n",
    "            loss_f=fair_loss(outputs[t], labels[t].to(dv),xc.to(dv))\n",
    "            if fg[t]==0:\n",
    "                loss_t=loss_a\n",
    "                lp[t]=0\n",
    "            else:\n",
    "                loss_t=loss_f\n",
    "                lp[t]=1\n",
    "            loss_t.backward(retain_graph=True)\n",
    "            \n",
    "            if init_loss_S[t]== None:\n",
    "                init_loss_S[t]=loss_t.item()\n",
    "            loss_ratio.append(loss_t.item()/init_loss_S[t])\n",
    "            grads_sh={}\n",
    "            for n,p in net.named_parameters():\n",
    "                if p.data.shape[0]!=2 and p.grad!=None:\n",
    "                    grads_sh[n] = p.grad\n",
    "                    p.grad=None\n",
    "            grads.append(grads_sh)\n",
    "            G_n.append(torch.linalg.norm(torch.stack([torch.linalg.norm(grads_sh[g]) for g in grads_sh])))\n",
    "        loss_pointer.append(lp)\n",
    "        G_n = torch.stack(G_n)\n",
    "        E_t = sum(loss_ratio)/len(loss_ratio)\n",
    "        r_t=[loss/E_t for loss in loss_ratio] #relative inverse trainin g rate of student\n",
    "        omega_S,net = Update_model(net,grads,omega_S,G_n,r_t,opti_S) \n",
    "        torch.save(net.state_dict(),path)\n",
    "        torch.save(omega_S,'/yourpath/ACS-PUMS/omega_S.pt')\n",
    "        for t in range(N_tasks):\n",
    "            state_net[t].append(torch.unsqueeze(torch.flatten(net.module.tasks_out[str(t)].weight),dim=0))\n",
    "\n",
    "            accuracy=acc(outputs[t].to(cpu),labels[t].to(cpu))\n",
    "            fairness=DM_rate(outputs[t].to(cpu), labels[t].to(cpu),xc.to(cpu))\n",
    "\n",
    "            learn_a=Student(stud,path,criteria,inputs.to(dv).float(),labels[t].to(dv),t=t)\n",
    "\n",
    "            learn_f=Student(stud,path,fair_loss,inputs.to(dv).float(),labels[t].to(dv),t=t,xc=xc.to(dv))\n",
    "\n",
    "\n",
    "            state_a[t].append(torch.unsqueeze(torch.flatten(learn_a.module.tasks_out[str(t)].weight),dim=0))            \n",
    "            state_f[t].append(torch.unsqueeze(torch.flatten(learn_f.module.tasks_out[str(t)].weight),dim=0))\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                out_A=learn_a(inputs.to(dv).float())[t]\n",
    "                out_F=learn_f(inputs.to(dv).float())[t]\n",
    "                acc_a=acc(out_A.to(cpu),labels[t].to(cpu))\n",
    "                DM_a=DM_rate(out_A.to(cpu),labels[t].to(cpu),xc.to(cpu))\n",
    "                acc_f=acc(out_F.to(cpu),labels[t].to(cpu))\n",
    "                DM_f=DM_rate(out_F.to(cpu),labels[t].to(cpu),xc.to(cpu))\n",
    "        \n",
    "            if best_S[t][0]==0:\n",
    "                best_S[t][0]=accuracy.item()\n",
    "                best_S[t][1]=fairness\n",
    "            # the reward functions\n",
    "            R_A[t].append(torch.min((acc_a-best_S[t][0])/best_S[t][0],(1-DM_a-best_S[t][1])/best_S[t][1]))\n",
    "            R_F[t].append(torch.min((acc_f-best_S[t][0])/best_S[t][0],(1-DM_f-best_S[t][1])/best_S[t][1]))\n",
    "        del inputs; del labels; del xc \n",
    "    for t in range(N_tasks):\n",
    "        state_net[t]=torch.cat(state_net[t],0).to(dv)\n",
    "        state_a[t]=torch.cat(state_a[t],0).to(dv)\n",
    "        state_f[t]=torch.cat(state_f[t],0).to(dv)\n",
    "        R_A[t]=torch.tensor(R_A[t]).to(dv)\n",
    "        R_F[t]=torch.tensor(R_F[t]).to(dv)\n",
    "    dqn=Teacher(state_net,state_a,state_f,R_A,R_F,dqn,opti_T)\n",
    "    torch.save(dqn.state_dict(),path_t)\n",
    "    del state_net; del state_a; del state_f; del R_A; del R_F\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        pred0=net(torch.tensor(X_val).to(dv).float())\n",
    "        print('Epoch: ',epoch, 'Loss pointers',loss_pointer)\n",
    "        action_choices.append(loss_pointer)\n",
    "        np.save(file=\"/yourpath/ACS-PUMS/loss_pointers_age_n.npy\",arr=np.array(action_choices)) \n",
    "        for t in range(N_tasks): \n",
    "            accuracy=acc(pred0[t].to(cpu),torch.tensor(y_v[t]))\n",
    "            EO=DM_rate(pred0[t].to(cpu),torch.tensor(y_v[t]),torch.tensor(g_val))\n",
    "            \n",
    "            if accuracy>best_S[t][0]:\n",
    "                best_S[t][0]=accuracy\n",
    "            if 1-EO>best_S[t][1]:\n",
    "                best_S[t][1]=1-EO\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[\n",
    "        'AGEP','SCHL','MAR','DIS','ESP','CIT','MIG', 'MIL',\n",
    "        'ANC','NATIVITY', 'DEAR', 'DEYE',\n",
    "        'DREM', 'SEX', 'RAC1P','PUMA','ST','OCCP',\n",
    "        'JWTRNS',#use 'JWTRNS' for testing (2019) data for training (2018) data the feature is 'JWTR',#\n",
    "        'POWPUMA',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folktables\n",
    "from folktables import ACSDataSource\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2019', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data( download=True)\n",
    "f, l1, g = Employment.df_to_numpy(acs_data)\n",
    "f, l2, g = Income.df_to_numpy(acs_data)\n",
    "''\n",
    "f, l3, g = HealthInsurance.df_to_numpy(acs_data)\n",
    "f, l4, g = TravelTime.df_to_numpy(acs_data)\n",
    "f, l5, g = IncomePovertyRatio.df_to_numpy(acs_data)\n",
    "y_test=np.array([[0 if v==False else 1 for v in l1],[0 if v==False else 1 for v in l2],[0 if v==False else 1 for v in l3],\\\n",
    "           [0 if v==False else 1 for v in l4],[0 if v==False else 1 for v in l5]])\n",
    "N_tasks=len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STL(nn.Module):\n",
    "\n",
    "    def __init__(self,d_in=50):\n",
    "        super(STL, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_in, 1024)  \n",
    "        self.bn1= nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fc4 = nn.Linear(1024,1024)        \n",
    "        self.task = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc4(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        t = self.task(x)\n",
    "       \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaths={'path'+str(t):'/yourpath/ACS_PUMS/model/Model_stl'+str(t)+'.pt' for t in range(N_tasks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net=nn.DataParallel(STL(d_in=X_train.shape[1]))\n",
    "net.to(dv)\n",
    "Bests=[]\n",
    "for t in range(N_tasks):  \n",
    "    net.load_state_dict(torch.load(spaths['path'+str(t)]))\n",
    "    pred0=net(torch.tensor(X_test).float())\n",
    "    accuracy=acc(pred0.to(cpu),torch.tensor(y_test[t]).to(cpu))\n",
    "    DM=DM_rate(pred0.to(cpu),torch.tensor(y_test[t]).to(cpu),torch.tensor(g_test).to(cpu))\n",
    "    Bests.append([accuracy,DM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0   Acc: tensor(0.9688) EOs: tensor(0.0096)\n",
      "Relative: tensor(0.9878) tensor(1.9025)\n",
      "Task 1   Acc: tensor(0.8262) EOs: tensor(0.0118)\n",
      "Relative: tensor(1.0279) tensor(1.8006)\n",
      "Task 2   Acc: tensor(0.8714) EOs: tensor(0.0009)\n",
      "Relative: tensor(1.0186) tensor(1.5337)\n",
      "Task 3   Acc: tensor(0.7437) EOs: tensor(0.0038)\n",
      "Relative: tensor(1.0327) tensor(1.4626)\n",
      "Task 4   Acc: tensor(0.7711) EOs: tensor(0.0002)\n",
      "Relative: tensor(1.0241) tensor(1.1760)\n"
     ]
    }
   ],
   "source": [
    "net=MTL(d_in=f.shape[1],tasks=N_tasks)\n",
    "net=nn.DataParallel(net)\n",
    "net.to(dv)\n",
    "l2t=[]\n",
    "net.load_state_dict(torch.load(path))\n",
    "pred0=net(torch.tensor(f).float())\n",
    "ra,rf=0,0\n",
    "for t in range(N_tasks):            \n",
    "    accuracy=acc(pred0[t],torch.tensor(y_test[t]))\n",
    "    DM=DM_rate(pred0[t],torch.tensor(y_test[t]),torch.tensor(g))\n",
    "    l2t.append([accuracy,DM])\n",
    "    ra+=accuracy/Bests[t][0]\n",
    "    rf+=DM/Bests[t][1]\n",
    "    print('Task',t,'  Acc:',accuracy, 'EOs:',DM)\n",
    "    print('Relative:',ra/(t+1), rf/(t+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
