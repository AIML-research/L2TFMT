{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy-FMT train on ACS-PUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL(nn.Module):\n",
    "\n",
    "    def __init__(self,d_in=50,tasks=2):\n",
    "        super(MTL, self).__init__()\n",
    "        self.tasks=tasks\n",
    "        self.fc1 = nn.Linear(d_in, 1024)  \n",
    "        self.bn1= nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fc4 = nn.Linear(1024,1024)\n",
    "        self.tasks_out=nn.ModuleDict({str(t):nn.Linear(128,2) for t in range(self.tasks)})\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc4(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))        \n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        t=[self.tasks_out[str(i)](x) for i in range(self.tasks)]\n",
    "        \n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fair_loss(output,target,x_control):\n",
    "    prot_att=x_control\n",
    "    index_prot=torch.squeeze(torch.nonzero(prot_att[:] != 1.))\n",
    "    target_prot=torch.index_select(target, 0, index=index_prot)\n",
    "    index_prot_pos=torch.squeeze(torch.nonzero(target_prot[:] == 1. ))\n",
    "    index_prot_neg=torch.squeeze(torch.nonzero(target_prot[:] == 0. ))\n",
    "\n",
    "    index_non_prot=torch.squeeze(torch.nonzero(prot_att[:] == 1.))\n",
    "    target_non_prot=torch.index_select(target, 0, index=index_non_prot)\n",
    "    index_non_prot_pos=torch.squeeze(torch.nonzero(target_non_prot[:] == 1. ))\n",
    "    index_non_prot_neg=torch.squeeze(torch.nonzero(target_non_prot[:] == 0. ))\n",
    "\n",
    "    l_prot_pos=F.cross_entropy(torch.index_select(output, 0, index=index_prot_pos),torch.index_select(target, 0, index=index_prot_pos))    \n",
    "    l_non_prot_pos=F.cross_entropy(torch.index_select(output, 0, index=index_non_prot_pos),torch.index_select(target, 0, index=index_non_prot_pos))    \n",
    "    l_non_prot_neg=F.cross_entropy(torch.index_select(output, 0, index=index_non_prot_neg),torch.index_select(target, 0, index=index_non_prot_neg))\n",
    "    l_prot_neg=F.cross_entropy(torch.index_select(output, 0, index=index_prot_neg),torch.index_select(target, 0, index=index_prot_neg))    \n",
    "\n",
    "    for l in [l_prot_pos,l_non_prot_pos,l_prot_neg,l_non_prot_neg]:\n",
    "        if torch.isinf(l)==True:\n",
    "            l=torch.zeros_like(l,requires_grad=True)\n",
    "    dl_pos=torch.max(l_prot_pos,l_non_prot_pos)\n",
    "    dl_neg=torch.max(l_prot_neg,l_non_prot_neg)\n",
    "    L=dl_pos+dl_neg\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "acc = torchmetrics.Accuracy()\n",
    "def DM_rate(output,target,x_control):\n",
    "    prot_att=x_control\n",
    "    index_prot=torch.squeeze(torch.nonzero(prot_att[:] != 1.))\n",
    "    target_prot=torch.index_select(target, 0, index=index_prot)\n",
    "    index_prot_pos=torch.squeeze(torch.nonzero(target_prot[:] == 1. ))\n",
    "    index_prot_neg=torch.squeeze(torch.nonzero(target_prot[:] == 0. ))\n",
    "\n",
    "    index_non_prot=torch.squeeze(torch.nonzero(prot_att[:] == 1.))\n",
    "    target_non_prot=torch.index_select(target, 0, index=index_non_prot)\n",
    "    index_non_prot_pos=torch.squeeze(torch.nonzero(target_non_prot[:] == 1. ))\n",
    "    index_non_prot_neg=torch.squeeze(torch.nonzero(target_non_prot[:] == 0. ))\n",
    "\n",
    "    if index_prot_pos.shape==torch.Size([]) or index_prot_pos.shape==torch.Size([0])\\\n",
    "        or index_non_prot_pos.shape==torch.Size([]) or index_non_prot_pos.shape==torch.Size([0]):\n",
    "            l_prot_pos=torch.tensor(0.0001)\n",
    "            l_non_prot_pos=torch.tensor(0.0001)\n",
    "    else:        \n",
    "            l_prot_pos=acc(torch.index_select(output, 0, index=index_prot_pos),torch.index_select(target, 0, index=index_prot_pos))    \n",
    "            l_non_prot_pos=acc(torch.index_select(output, 0, index=index_non_prot_pos),torch.index_select(target, 0, index=index_non_prot_pos))    \n",
    "    \n",
    "    if index_prot_neg.shape==torch.Size([]) or index_prot_neg.shape==torch.Size([0])\\\n",
    "        or index_non_prot_neg.shape==torch.Size([]) or index_non_prot_neg.shape==torch.Size([0]):\n",
    "            l_prot_neg=torch.tensor(0.0001)\n",
    "            l_non_prot_neg=torch.tensor(0.0001)\n",
    "    else:        \n",
    "            l_prot_neg=acc(torch.index_select(output, 0, index=index_prot_neg),torch.index_select(target, 0, index=index_prot_neg))    \n",
    "            l_non_prot_neg=acc(torch.index_select(output, 0, index=index_non_prot_neg),torch.index_select(target, 0, index=index_non_prot_neg))  \n",
    "            \n",
    "    dl_pos=torch.abs(l_prot_pos-l_non_prot_pos)\n",
    "    dl_neg=torch.abs(l_prot_neg-l_non_prot_neg)\n",
    "    DM=dl_pos+dl_neg\n",
    "    \n",
    "    return DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=MTL(model_pre.to(dv),tasks=N_tasks)\n",
    "net=nn.DataParallel(net)\n",
    "net.to(dv)\n",
    "opti_S=optim.AdamW(params=net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "omega_S=torch.tensor([1/N_tasks for t in range(N_tasks)])\n",
    "init_loss_S=[None for t in range(N_tasks)]\n",
    "m_acc,flag=0.0,1\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "loss_pointer=[1]\n",
    "best_S=[[0,0] for t in range(N_tasks)]\n",
    "optim_path,optim_path_disc,optim_clas=[],[],[]#[[] for p in x_control]\n",
    "path='/yourpath/model/Vanilla_MTL.pt'\n",
    "ML_AF_GN=[[] for t in range(N_tasks)]\n",
    "torch.save(net.state_dict(),path)\n",
    "#net.load_state_dict(torch.load(path))\n",
    "for epoch in range(50):  # loop over each NN multiple times\n",
    "    i,batch=0,8192\n",
    "    j=0\n",
    "    while(i<len(X_train)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        if (i+batch)<len(X_train):\n",
    "            inputs, in_t = torch.tensor(X_train[i:i+batch]),in_tr[i:i+batch]\n",
    "            labels=[y_train[t][i:i+batch] for t in range(N_tasks)]\n",
    "            #if epoch<pretrn:\n",
    "            xc=xg[i:i+batch]\n",
    "            i=i+batch \n",
    "        else:\n",
    "            inputs,in_t = torch.tensor(X_train[i:]),in_tr[i:]\n",
    "            labels=[y_train[t][i:] for t in range(N_tasks)]\n",
    "            #if epoch<pretrn:\n",
    "            xc=xg[i:]\n",
    "            i=len(X_train)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        #net.load_state_dict(torch.load(path))           \n",
    "        opti_S.zero_grad()\n",
    "        outputs = net(inputs.to(dv).float())\n",
    "        lp,grads,G_n,loss_ratio=[0 for t in range(N_tasks)],[],[],[]\n",
    "        for t in range(N_tasks):\n",
    "            loss_a=criteria(outputs[t], labels[t].to(dv))\n",
    "            loss_f=fair_loss(outputs[t], labels[t].to(dv),xc.to(dv))\n",
    "            if loss_a>loss_f:\n",
    "                loss_t=loss_a\n",
    "            else:\n",
    "                loss_t=loss_f\n",
    "            loss_t.backward(retain_graph=True)\n",
    "            \n",
    "            if init_loss_S[t]== None:\n",
    "                init_loss_S[t]=loss_t.item()\n",
    "            loss_ratio.append(loss_t.item()/init_loss_S[t])\n",
    "            grads_sh={}\n",
    "            for n,p in net.named_parameters():\n",
    "                if p.data.shape[0]!=2 and p.grad!=None:\n",
    "                    grads_sh[n] = p.grad\n",
    "                    p.grad=None\n",
    "                    #p.grad.zero_()\n",
    "            grads.append(grads_sh)\n",
    "            G_n.append(torch.linalg.norm(torch.stack([torch.linalg.norm(grads_sh[g]) for g in grads_sh])))\n",
    "        G_n = torch.stack(G_n)\n",
    "        #print('GN:',G_n)\n",
    "        E_t = sum(loss_ratio)/len(loss_ratio)\n",
    "        r_t=[loss/E_t for loss in loss_ratio] #relative inverse trainin g rate of student\n",
    "        omega_S,net = Update_model(net,grads,omega_S,G_n,r_t,opti_S) \n",
    "        torch.save(net.state_dict(),path)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        pred0=net(torch.tensor(X_val).to(device).float())\n",
    "        print('Epoch: ',epoch)\n",
    "        for t in range(N_tasks):            \n",
    "            accuracy=acc(pred0[t],y_v[t])\n",
    "            EO=DM_rate(pred0[t],torch.tensor(y_v[t]),torch.tensor(g_val))\n",
    "            ML_AF_GN[t].append([accuracy,EO])\n",
    "            np.save('/home/roy/model/PUMS_GN_AF.npy',ML_AF_GN)\n",
    "            print('Task',t,'  Acc:',accuracy, 'EO:',EO)#'Loss pointers',loss_pointer[-3:])\n",
    "            if accuracy>best_S[t][0]:\n",
    "                best_S[t][0]=accuracy\n",
    "            if 1-EO>best_S[t][1]:\n",
    "                best_S[t][1]=1-EO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy-FMT train on CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as pretrain\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre=pretrain.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in model_pre.parameters():\n",
    "    par.requires_grad=False\n",
    "in_fc=model_pre.fc.in_features\n",
    "model_pre.fc=nn.Linear(in_fc,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL(nn.Module):\n",
    "\n",
    "    def __init__(self,pretrain,tasks=2):\n",
    "        super(MTL, self).__init__()\n",
    "        self.tasks=tasks\n",
    "        self.fc1 = pretrain\n",
    "        self.bn1= nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.fc4 = nn.Linear(1024,1024)\n",
    "        self.tasks_out=nn.ModuleDict({str(t):nn.Linear(128,2) for t in range(self.tasks)})\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc4(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))        \n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        t=[self.tasks_out[str(i)](x) for i in range(self.tasks)]\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "def Update_model(model,grads_sh,omega,G_n,r_t,opti):\n",
    "    loss_gn=[(G_n[t]-torch.mean(G_n)*r_t[t]) for t in range(len(G_n))]\n",
    "    for i in range(len(G_n)):\n",
    "        d_l=0\n",
    "        if loss_gn[i]>0:\n",
    "            d_l+=(len(G_n)-1)/len(G_n)*G_n[i]\n",
    "        elif loss_gn[i]<0:\n",
    "            d_l-=(len(G_n)-1)/len(G_n)*G_n[i]\n",
    "        for j in range(len(G_n)):\n",
    "            if j!=i:\n",
    "                if loss_gn[j]>0:\n",
    "                    d_l-=(G_n[i]/len(G_n))\n",
    "                elif loss_gn[j]<0:\n",
    "                    d_l+=(G_n[i]/len(G_n))\n",
    "        \n",
    "        omega[i]-=lr*d_l\n",
    "    for t in range(len(G_n)):\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.data.shape[0]!=2 and p.grad!=None:\n",
    "                if t==0:\n",
    "                    p.grad=omega[t]*grads_sh[t][n]\n",
    "                else:\n",
    "                    p.grad+=omega[t]*grads_sh[t][n]\n",
    "                    \n",
    "    opti.step() \n",
    "    total=sum(omega)\n",
    "    for i in range(len(omega)):\n",
    "        omega[i]=omega[i]/total\n",
    "    return omega,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=MTL(model_pre.to(dv),tasks=N_tasks)\n",
    "net=nn.DataParallel(net)\n",
    "net.to(dv)\n",
    "opti_S=optim.AdamW(params=net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_S=torch.tensor([1/N_tasks for t in range(N_tasks)]).to(dv)\n",
    "init_loss_S=[None for t in range(N_tasks)]\n",
    "m_acc,flag=0.0,1\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optim_path,optim_path_disc,optim_clas=[],[],[]#[[] for p in x_control]\n",
    "path='/yourpath/CelebA/model/Vanilla_MTL.pt'\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "dt=[dataloader1,dataloader2,dataloader3,dataloader4]\n",
    "yt=[y_tr1,y_tr2,y_tr3,y_tr4]\n",
    "gt=[g_tr1,g_tr2,g_tr3,g_tr4]\n",
    "for epoch in range(20):  # loop over each NN multiple times\n",
    "    loss_pointer=[]\n",
    "    state_net,state_a,state_f=[[] for t in range(N_tasks)],[[] for t in range(N_tasks)],[[] for t in range(N_tasks)]\n",
    "    R_A,R_F=[[] for t in range(N_tasks)],[[] for t in range(N_tasks)]\n",
    "    ch=np.random.choice([0,1,2,3])\n",
    "    dataloader,y_train,g_train=dt[ch],yt[ch],gt[ch]\n",
    "    y_train=[torch.tensor(y_train[i]) for i in range(N_tasks)]\n",
    "    xg=torch.tensor(g_train)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs =data[0]\n",
    "        if ((i+1)*batch_size)<len(in_tr):            \n",
    "            labels=[y_train[t][i*batch_size:(i+1)*batch_size] for t in range(N_tasks)]\n",
    "            xc=xg[i*batch_size:(i+1)*batch_size]\n",
    "        else:\n",
    "            labels=[y_train[t][i*batch_size:] for t in range(N_tasks)]\n",
    "            xc=xg[i*batch_size:]\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        net.load_state_dict(torch.load(path))\n",
    "        net.to(dv)\n",
    "                       \n",
    "        opti_S.zero_grad()\n",
    "        outputs = net(inputs.to(dv).float())\n",
    "        grads,G_n,loss_ratio=[],[],[]\n",
    "        for t in range(N_tasks):\n",
    "            \n",
    "            #MTL starts\n",
    "            loss_a=criteria(outputs[t], labels[t].to(dv))\n",
    "            loss_f=fair_loss(outputs[t], labels[t].to(dv),xc.to(dv))\n",
    "            if loss_a>loss_f:\n",
    "                loss_t=loss_a\n",
    "            else:\n",
    "                loss_t=loss_f\n",
    "            loss_t.backward(retain_graph=True)\n",
    "            \n",
    "            if init_loss_S[t]== None:\n",
    "                init_loss_S[t]=loss_t.item()\n",
    "            loss_ratio.append(loss_t.item()/init_loss_S[t])\n",
    "            grads_sh={}\n",
    "            for n,p in net.named_parameters():\n",
    "                if p.data.shape[0]!=2 and p.grad!=None:\n",
    "                    grads_sh[n] = p.grad\n",
    "                    p.grad=None\n",
    "                    #p.grad.zero_()\n",
    "            grads.append(grads_sh)\n",
    "            G_n.append(torch.linalg.norm(torch.stack([torch.linalg.norm(grads_sh[g]) for g in grads_sh])))\n",
    "        G_n = torch.stack(G_n)\n",
    "        E_t = sum(loss_ratio)/len(loss_ratio)\n",
    "        r_t=[loss/E_t for loss in loss_ratio] #relative inverse trainin g rate of student\n",
    "        omega_S,net = Update_model(net,grads,omega_S,G_n,r_t,opti_S) \n",
    "        torch.save(net.state_dict(),path)\n",
    "        torch.save(omega_S,'/yourpath/CelebA/model/omega.pt')\n",
    "  \n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        #Val_mod.load_state_dict(torch.load(path))\n",
    "        \n",
    "        ch=np.random.choice([0,1])\n",
    "        val_batch,y_v,g_val=next(iter(d_v[ch]))[0],yv[ch],gv[ch]\n",
    "        pred0=net(val_batch.to(dv))#=Val_mod\n",
    "        print('Epoch: ',epoch)\n",
    "        for t in range(N_tasks): \n",
    "            accuracy=acc(pred0[t].to(cpu),torch.tensor(y_v[t]))\n",
    "            EO=DM_rate(pred0[t].to(cpu),torch.tensor(y_v[t]),torch.tensor(g_val))\n",
    "            \n",
    "            print('Task',t,'  Acc:',accuracy, 'EO:',EO)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
